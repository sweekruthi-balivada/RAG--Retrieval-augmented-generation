# RAG-Retrieval-augmented-generation
RAG, LLMs and Langchain

RAG, or Retrieval-Augmented Generation, stands as a groundbreaking advancement in natural language processing, seamlessly integrating retrieval-based and generation-based models. By amalgamating these approaches, RAG possesses the capability to sift through extensive textual datasets, extracting pertinent information, and subsequently synthesizing responses imbued with contextual relevance and creativity. Its utility extends across various domains, from enhancing question-answering systems to refining information retrieval mechanisms. For individuals deeply immersed in the realms of deep learning and NLP, RAG represents a paradigm shift, offering unparalleled potential for advancing both research and practical applications in the field.

**For the development of RAG, there are 3 important steps:**<br>
1. Chunking: Split your data be it a text, code, or documents. The chunk size matters!
2. Embeddings: Embeddings are the numerical representations of the data. Store the embeddings in Fiass or ChromaDB. Here I have used Fiass Database.
3. Model: Choose a LLM model and pass your data.<br>
Thats it! Congratulations! You have your own RAG!

